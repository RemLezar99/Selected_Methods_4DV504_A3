{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e79d19",
   "metadata": {},
   "source": [
    "# Task 1 – Medical Named Entity Recognition (NER)\n",
    "\n",
    "In this notebook, we fine-tune encoder-only transformer models for a medical Named Entity Recognition (NER) task on Swedish text. \n",
    "\n",
    "The dataset used in this task consists of Swedish medical text from the 1177 Vårdguiden subset. The goal is to identify and classify medical entities such as disorders, pharmaceutical drugs, and body structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8610fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d5982e",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "The dataset is loaded from the Hugging Face Datasets library. Only the 1177 Vårdguiden subset is used, as specified in the assignment instructions. This dataset contains medical text annotated with entity spans and entity types.\n",
    "\n",
    "Before training, the dataset is inspected and split into training, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e67b39",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "The dataset is split following the prescribed setup:\n",
    "- 80% of the data is used for training and validation\n",
    "- 20% is held out as a test set and used only for final evaluation\n",
    "- The training–validation portion is further split into 90% training data and 10% validation data\n",
    "\n",
    "This ensures that the test set remains fully isolated during model development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa307bd6-8494-4541-8ecf-870358e946ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22368ba3d9c346feb0f43dd25d793ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b76a0709151471f9a71635fe8d30e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/77.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e505b626d64759a5b2f1cf8134a609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/927 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "927\n",
      "dict_keys(['train', 'validation', 'test'])\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"community-datasets/swedish_medical_ner\", '1177')\n",
    "\n",
    "full_dataset = ds[\"train\"]\n",
    "print(len(full_dataset))\n",
    "full_ds = ds[\"train\"]  # use all examples\n",
    "\n",
    "split_80_20 = full_ds.train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=67\n",
    ")\n",
    "\n",
    "trainval_ds = split_80_20[\"train\"]  # 80%\n",
    "test_ds     = split_80_20[\"test\"]   # 20% (final test set)\n",
    "\n",
    "split_90_10 = trainval_ds.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=67\n",
    ")\n",
    "\n",
    "train_ds = split_90_10[\"train\"]  # 72%\n",
    "val_ds   = split_90_10[\"test\"]   # 8%\n",
    "\n",
    "final_ds = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"validation\": val_ds,\n",
    "    \"test\": test_ds\n",
    "})\n",
    "\n",
    "print(final_ds.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead6925",
   "metadata": {},
   "source": [
    "## Tagging Schemes\n",
    "\n",
    "To train a token-level NER model, entity span annotations must be converted into token-level labels. Two standard tagging schemes are used in this notebook:\n",
    "\n",
    "- **BIO (Beginning–Inside–Outside)**: Marks the beginning of an entity, continuation inside an entity, and non-entity tokens.\n",
    "- **BIOES (Beginning–Inside–Outside–End–Single)**: Extends BIO by explicitly marking the end of multi-token entities and single-token entities.\n",
    "\n",
    "Both tagging schemes are applied to the same underlying data in order to compare their impact on model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d91891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_MAP = {\n",
    "    0: \"DISORDER\",\n",
    "    1: \"DRUG\",\n",
    "    2: \"BODY\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29515d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_RE = re.compile(r\"\\S+\")\n",
    "\n",
    "def words_with_offsets(text):\n",
    "    words, offsets = [], []\n",
    "    for match in WORD_RE.finditer(text):\n",
    "        words.append(match.group())\n",
    "        offsets.append((match.start(), match.end()))\n",
    "    return words, offsets\n",
    "\n",
    "\n",
    "def normalize_entities(entities):\n",
    "    \"\"\"\n",
    "    Convert dataset-style entity dict-of-lists into list-of-dicts.\n",
    "    \"\"\"\n",
    "    starts = entities[\"start\"]\n",
    "    ends = entities[\"end\"]\n",
    "    types = entities[\"type\"]\n",
    "\n",
    "    normalized = []\n",
    "    for s, e, t in zip(starts, ends, types):\n",
    "        normalized.append({\n",
    "            \"start\": int(s),\n",
    "            \"end\": int(e),\n",
    "            \"type\": int(t)\n",
    "        })\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9acccb0",
   "metadata": {},
   "source": [
    "## Tokenization and Label Alignment\n",
    "\n",
    "The pretrained transformer models use subword tokenization, which means that a single word can be split into multiple subword tokens. To handle this, entity labels are aligned with the tokenized output.\n",
    "\n",
    "Only the first subword token of each word is assigned a label, while subsequent subword tokens and special tokens are assigned an ignore label. This ensures that the loss is computed correctly during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51f27b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans_to_word_labels(text, entities, scheme=\"BIO\"):\n",
    "    words, offsets = words_with_offsets(text)\n",
    "    labels = [\"O\"] * len(words)\n",
    "\n",
    "    entities = normalize_entities(entities)\n",
    "\n",
    "    for ent in entities:\n",
    "        start, end = ent[\"start\"], ent[\"end\"]\n",
    "        ent_type = TYPE_MAP[ent[\"type\"]]\n",
    "\n",
    "        covered = [\n",
    "            i for i, (ws, we) in enumerate(offsets)\n",
    "            if not (we <= start or ws >= end)\n",
    "        ]\n",
    "\n",
    "        if not covered:\n",
    "            continue\n",
    "\n",
    "        if scheme == \"BIO\":\n",
    "            labels[covered[0]] = f\"B-{ent_type}\"\n",
    "            for i in covered[1:]:\n",
    "                labels[i] = f\"I-{ent_type}\"\n",
    "\n",
    "        elif scheme == \"BIOES\":\n",
    "            if len(covered) == 1:\n",
    "                labels[covered[0]] = f\"S-{ent_type}\"\n",
    "            else:\n",
    "                labels[covered[0]] = f\"B-{ent_type}\"\n",
    "                for i in covered[1:-1]:\n",
    "                    labels[i] = f\"I-{ent_type}\"\n",
    "                labels[covered[-1]] = f\"E-{ent_type}\"\n",
    "\n",
    "    return words, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "626ed9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_label_list(scheme):\n",
    "    entity_types = [\"DISORDER\", \"DRUG\", \"BODY\"]\n",
    "    if scheme == \"BIO\":\n",
    "        return [\"O\"] + [f\"{p}-{t}\" for t in entity_types for p in [\"B\", \"I\"]]\n",
    "    else:  # BIOES\n",
    "        return [\"O\"] + [f\"{p}-{t}\" for t in entity_types for p in [\"B\", \"I\", \"E\", \"S\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "125f9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(words, word_labels, tokenizer, label2id):\n",
    "    enc = tokenizer(\n",
    "        words,\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    word_ids = enc.word_ids()\n",
    "    labels = []\n",
    "    prev_word_id = None\n",
    "\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            labels.append(-100)\n",
    "        elif word_id != prev_word_id:\n",
    "            labels.append(label2id[word_labels[word_id]])\n",
    "        else:\n",
    "            labels.append(-100)\n",
    "        prev_word_id = word_id\n",
    "\n",
    "    enc[\"labels\"] = labels\n",
    "    return enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a809db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset_dict, tokenizer, scheme):\n",
    "    label_list = build_label_list(scheme)\n",
    "    label2id = {l: i for i, l in enumerate(label_list)}\n",
    "\n",
    "    def preprocess(example):\n",
    "        words, labels = spans_to_word_labels(\n",
    "            example[\"sentence\"],\n",
    "            example[\"entities\"],\n",
    "            scheme=scheme\n",
    "        )\n",
    "        return tokenize_and_align_labels(words, labels, tokenizer, label2id)\n",
    "\n",
    "    encoded = DatasetDict()\n",
    "    for split in dataset_dict.keys():\n",
    "        encoded[split] = dataset_dict[split].map(preprocess)\n",
    "\n",
    "    return encoded, label_list, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e9f3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred, id2label):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_preds, true_labels = [], []\n",
    "\n",
    "    for pred, lab in zip(predictions, labels):\n",
    "        p_seq, l_seq = [], []\n",
    "        for p, l in zip(pred, lab):\n",
    "            if l == -100:\n",
    "                continue\n",
    "            p_seq.append(id2label[p])\n",
    "            l_seq.append(id2label[l])\n",
    "        true_preds.append(p_seq)\n",
    "        true_labels.append(l_seq)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_preds),\n",
    "        \"recall\": recall_score(true_labels, true_preds),\n",
    "        \"f1\": f1_score(true_labels, true_preds),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12d9b56",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Models are fine-tuned using the Hugging Face Trainer API. Training is performed with fixed hyperparameters across experiments, except for the tagging scheme and learning rate when conducting the optional extension.\n",
    "\n",
    "GPU acceleration and mixed-precision training are used when available to improve training efficiency. Model performance is monitored on the validation set, and the best model is selected based on F1-score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a319866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ner_model(model_name, scheme, learning_rates=[2e-5]):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    encoded_ds, label_list, label2id = preprocess_dataset(\n",
    "        final_ds, tokenizer, scheme\n",
    "    )\n",
    "\n",
    "    id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "    lr_results = {}\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nTraining {model_name} | {scheme} | lr={lr}\")\n",
    "\n",
    "        model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(label_list),\n",
    "            id2label=id2label,\n",
    "            label2id=label2id\n",
    "        )\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=f\"./outputs/ner_{model_name.replace('/', '_')}_{scheme}_lr{lr}\",\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=5,\n",
    "            weight_decay=0.01,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            logging_steps=50,\n",
    "            fp16=True,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=encoded_ds[\"train\"],\n",
    "            eval_dataset=encoded_ds[\"validation\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "            compute_metrics=lambda p: compute_metrics(p, id2label)\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        test_metrics = trainer.evaluate(encoded_ds[\"test\"])\n",
    "\n",
    "        lr_results[lr] = test_metrics\n",
    "\n",
    "    return lr_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9d4367",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Two encoder-only transformer models are evaluated in this notebook:\n",
    "- A Swedish-specific BERT model trained on Swedish text\n",
    "- A multilingual BERT model trained on text from multiple languages, including Swedish\n",
    "\n",
    "These models are selected to compare language-specific pretraining with multilingual pretraining in a domain-specific, low-resource setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb74ad",
   "metadata": {},
   "source": [
    "## Optional Extension: Learning Rate Comparison\n",
    "\n",
    "As an optional extension, the effect of different learning rates on NER performance is evaluated. Learning rates of 1e-5, 2e-5, and 5e-5 are tested across model and tagging scheme configurations.\n",
    "\n",
    "This experiment helps analyze how optimization settings influence convergence and final performance for a token-level sequence labeling task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82517db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training KB/bert-base-swedish-cased | Scheme: BIO ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b9cd000b4b4177a4913d52e5cbd692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training KB/bert-base-swedish-cased | BIO | lr=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_2600\\1965683095.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.103624</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.877193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.012841</td>\n",
       "      <td>0.987805</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>0.003806</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037700</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training KB/bert-base-swedish-cased | BIO | lr=2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_2600\\1965683095.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.016062</td>\n",
       "      <td>0.940476</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.957576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.543500</td>\n",
       "      <td>0.003522</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.021200</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training KB/bert-base-swedish-cased | BIO | lr=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_2600\\1965683095.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.026354</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.950617</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.344400</td>\n",
       "      <td>0.023088</td>\n",
       "      <td>0.987805</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.987805</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training KB/bert-base-swedish-cased | Scheme: BIOES ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e42f10447646de80c0a9f3d25079b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/666 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7e9362fff94b5c899e3615b1b9018c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ee74f7b963499688cbf8ad6e545506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/186 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training KB/bert-base-swedish-cased | BIOES | lr=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_2600\\1965683095.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.205239</td>\n",
       "      <td>0.590476</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.048800</td>\n",
       "      <td>0.029451</td>\n",
       "      <td>0.941860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.111300</td>\n",
       "      <td>0.009235</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.005224</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training KB/bert-base-swedish-cased | BIOES | lr=2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_2600\\1965683095.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.030930</td>\n",
       "      <td>0.918605</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.946108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.704800</td>\n",
       "      <td>0.005036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.037700</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at KB/bert-base-swedish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training KB/bert-base-swedish-cased | BIOES | lr=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_2600\\1965683095.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.037248</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.963855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.440800</td>\n",
       "      <td>0.023933</td>\n",
       "      <td>0.987805</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training google-bert/bert-base-multilingual-cased | Scheme: BIO ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c331d72be617474c809c62108f6e1182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/666 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a571ab5c302490089fefb35ae20c073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601aa6b5eaa34fc1a877288191665965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/186 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training google-bert/bert-base-multilingual-cased | BIO | lr=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_2600\\1965683095.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.134720</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.950617</td>\n",
       "      <td>0.939024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.635300</td>\n",
       "      <td>0.022325</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.969325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.008056</td>\n",
       "      <td>0.940476</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.957576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.005911</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.975610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.005599</td>\n",
       "      <td>0.987805</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training google-bert/bert-base-multilingual-cased | BIO | lr=2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_2600\\1965683095.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.030081</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.432400</td>\n",
       "      <td>0.005663</td>\n",
       "      <td>0.951807</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.963415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training google-bert/bert-base-multilingual-cased | BIO | lr=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_2600\\1965683095.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.015753</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.969697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.285000</td>\n",
       "      <td>0.008531</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.981595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training google-bert/bert-base-multilingual-cased | Scheme: BIOES ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0beb29b175b47509e9859fa2d0540bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/666 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567fbba2c60a4b0386d994ba8322bafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/75 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94def7acb7224022939efe0534d5efa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/186 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training google-bert/bert-base-multilingual-cased | BIOES | lr=1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_2600\\1965683095.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.244654</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>0.753086</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.870800</td>\n",
       "      <td>0.047609</td>\n",
       "      <td>0.939024</td>\n",
       "      <td>0.950617</td>\n",
       "      <td>0.944785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.016838</td>\n",
       "      <td>0.929412</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.951807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.011802</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>0.975610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.010089</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training google-bert/bert-base-multilingual-cased | BIOES | lr=2e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_2600\\1965683095.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>0.917647</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.939759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.604500</td>\n",
       "      <td>0.011392</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.945455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.004580</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training google-bert/bert-base-multilingual-cased | BIOES | lr=5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_2600\\1965683095.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 00:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.030166</td>\n",
       "      <td>0.987805</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.385500</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.934132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [\n",
    "    \"KB/bert-base-swedish-cased\",\n",
    "    \"google-bert/bert-base-multilingual-cased\"\n",
    "]\n",
    "\n",
    "schemes = [\"BIO\", \"BIOES\"]\n",
    "\n",
    "learning_rates = [1e-5, 2e-5, 5e-5]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name in models:\n",
    "    for scheme in schemes:\n",
    "        print(f\"\\n=== Training {model_name} | Scheme: {scheme} ===\")\n",
    "\n",
    "        lr_results = train_ner_model(\n",
    "            model_name=model_name,\n",
    "            scheme=scheme,\n",
    "            learning_rates=learning_rates\n",
    "        )\n",
    "\n",
    "        # Store results with explicit keys\n",
    "        for lr, metrics in lr_results.items():\n",
    "            results[(model_name, scheme, lr)] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df6ca8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (model, scheme), metrics \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Scheme: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscheme\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for (model, scheme, lr), metrics in results.items():\n",
    "    print(f\"\\nModel: {model} | Scheme: {scheme} | Learning rate: {lr}\")\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "        else:\n",
    "            print(f\"{k}: {v}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b6d47",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Model performance is evaluated on the held-out test set using standard NER metrics: accuracy, precision, recall, and F1-score. The test set is used only once per configuration, after training is complete.\n",
    "\n",
    "Results are collected and stored for later analysis and inclusion in the final report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff3f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "\n",
    "for (model, scheme, lr), metrics in results.items():\n",
    "    rows.append({\n",
    "        \"Model\": model,\n",
    "        \"Tagging Scheme\": scheme,\n",
    "        \"Learning Rate\": lr,\n",
    "        \"Precision\": metrics[\"eval_precision\"],\n",
    "        \"Recall\": metrics[\"eval_recall\"],\n",
    "        \"F1\": metrics[\"eval_f1\"]\n",
    "    })\n",
    "\n",
    "ner_results_df = pd.DataFrame(rows)\n",
    "ner_results_df.sort_values([\"Model\", \"Tagging Scheme\", \"Learning Rate\"])\n",
    "\n",
    "ner_results_df.to_csv(\"results/ner_results_with_lr.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
